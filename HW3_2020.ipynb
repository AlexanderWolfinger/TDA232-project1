{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ledX1hQo5kXC"
   },
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 232 Machine Learning: Home Assignment 3 -- Classification (20 points)** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: K-Nearest-Neighbour (Y), Naive-bayes Classifier (D), Support Vector Machine (D), Logistic Regression (Y)**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Yuchong (Y), Divya (D)** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 6th May** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Name, Personal No., Email** <br />\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "General guidelines:\n",
    "*   All solutions to theoretical and pratical problems must be submitted in this ipynb notebook, and equations wherever required, should be formatted using LaTeX math-mode.\n",
    "*   All discussion regarding practical problems, along with solutions and plots should be specified in this notebook. All plots/results should be visible such that the notebook do not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.\n",
    "*   Your name, personal number and email address should be specified above.\n",
    "*   All tables and other additional information should be included in this notebook.\n",
    "*   **All the answers for theoretical questions must be filled in the cells created for you with \"Your answer here\" below each question, but feel free to add more cells if needed.**\n",
    "*   Before submitting, make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbqQD9bj6HzP"
   },
   "source": [
    "\n",
    "# Theoretical Questions\n",
    "## 1. K-Nearest-Neighbour Classification (4 pts)\n",
    "### 1.1 Exercise 1 (2 pts)\n",
    "A KNN classifier assigns a test instance the majority class associated with its K nearest training instances. Distance between instances is measured using Euclidean distance. Suppose we have the following training set of positive (+) and negative (-) instances and a single test instance (o). All instances are projected onto a vector space of two real-valued features (X and Y). Answer the following questions. Assume “unweighted” KNN (every nearest neighbor contributes equally to the final vote).\n",
    "\n",
    "![替代文字](https://raw.githubusercontent.com/BruceZHANG6/Imagesforuse/master/knn2.png)\n",
    "\n",
    "a) What would be the class assigned to this test instance for K=1, K=5, K=7 and why? (**1 pt**)\n",
    "\n",
    "b) The classification result is affected by the increasing K, so what will be the maxinum value of K you think in this case? Why? (**1 pt**)\n",
    "(**Hint: After K reaches a certain value, the classification result will not change. Find the value!**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0xnH14a0rsj"
   },
   "source": [
    "### Your answer here:\n",
    "a) From the figure above we can see that the 3 nearest neighbors are of the class (-) and the next 4 nearest neighbors (4-7) are of the class (+), this gives us that the class assignment for the test instance will be:\n",
    "\n",
    "$K=1$: (-), since 1(-) > 0(+)\n",
    "\n",
    "$K=5$: (-), since 3(-) > 2(+)\n",
    "\n",
    "$K=7$: (+), since 3(-) < 4(+)\n",
    "\n",
    "\n",
    "b) Since the total intances of (-) in the data set above is 6, all of these are included in $K=12$ so for  $K\\geq13$ all new neighbors will be of the class (+), it will not change the class assigment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqgZRVS80xr7"
   },
   "source": [
    "### 1.2 Exercise 2 (2 pts)\n",
    "Consider 5 data points:\n",
    "\n",
    "$$\\{({0},{1}), ({-1},{0})\\}∈ Class1,$$ \n",
    "\n",
    "$$\\{({1},{0}), ({0},{-1}), (-\\frac{1}{2}, \\frac{1}{2})\\}∈ Class2.$$\n",
    "\n",
    "Consider two test data points:\n",
    "\n",
    "$$(-\\frac{3}{4}, \\frac{3}{4})∈ Class1, (\\frac{1}{2}, \\frac{1}{2})∈ Class2$$\n",
    "\n",
    "Compute the **probability of error** based on k-nearest neighbor rule when $ K=\\{1, 2, 3, 4, 5\\}$ and explain why.\n",
    "(**Hint: The probability of error is the probability of one point is misclassified times the probability of another point is also misclassified**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pK-7HuAB0ztS"
   },
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8t9u_kDpgTD2"
   },
   "source": [
    "## 2. [Naive Bayes Classifier, 6 points]\n",
    "\n",
    "### Exercise 2.1 (3 pts)\n",
    "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
    "$$\n",
    "x = (\\mbox{rich, married, healthy}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
    "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
    "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
    "if they’re not. The following responses were obtained.\n",
    "\n",
    "$$\n",
    "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 0) \\\\\n",
    "c = 0: (0, 0, 0),(1, 0, 1),(0, 0, 1),(0, 1, 0)\n",
    "$$\n",
    "\n",
    "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''? **(1 pt)**\n",
    "\n",
    "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'') **(2 pts)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEEeDnaN1Ikp"
   },
   "source": [
    "### Your answer here:\n",
    "We will use Bayes rule:\n",
    "$$\n",
    "P(t_{new} = k | \\mathbf{X}, \\mathbf{t}, \\mathbf{x}_{new}) = \\frac{P(\\mathbf{x}_{new} | \\mathbf{t}_{new} = k, \\mathbf{X}, \\mathbf{t}) P(\\mathbf{t}_{new} = k)}{\\sum_j p(\\mathbf{x}_{new} | \\mathbf{t}_{new} = j, \\mathbf{X}, \\mathbf{t}) P(t_{new} = j)},\n",
    "$$\n",
    "where the ''naive Bayes'' assumption is that the joint distribution of the likelihood is the product of the individual likelihoods:\n",
    "$$\n",
    "p(\\mathbf{x}_{new} | \\mathbf{t}_{new} = j, \\mathbf{X}, \\mathbf{t}) = \\prod_{d=1}^D p(x_d^{new} | \\mathbf{t}_{new} = j, \\mathbf{X}, \\mathbf{t}).\n",
    "$$\n",
    "In this case $P(t_{new} = k)$ corresponds to $P(c = k)$ for $k \\in \\{0,1\\}$. We have 8 cases in our data set and equal amount of content and not content persons in both cases wich gives us $P(c = 0) = P(c = 1) = \\frac{4}{8} = \\frac{1}{2}$.\n",
    "\n",
    "1.\n",
    "In the first question it is given that $\\mathbf{x}_{new} = (0,1,1)$ and $c = 1$.\n",
    "\n",
    "This gives us that:\n",
    "$$\n",
    "P(\\text{Not rich} | c = 0) = \\frac{1 + 0 + 1 + 1}{4} = \\frac{3}{4},\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Not rich} | c = 1) = \\frac{0 + 1 + 0 + 0}{4} = \\frac{1}{4},\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Married} | c = 0) = \\frac{1 + 0 + 0 + 1}{4} = \\frac{1}{4},\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Married} | c = 1) = \\frac{1 + 0 + 1 + 0}{4} = \\frac{2}{4} = \\frac{1}{2},\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Healthy} | c = 0) = \\frac{0 + 1 + 1 + 0}{4} = \\frac{2}{4} = \\frac{1}{2},\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Healthy} | c = 1) = \\frac{1 + 1 + 0 + 0}{4} = \\frac{2}{4} = \\frac{1}{2}.\n",
    "$$\n",
    "\n",
    "This gives us all we need for the full equation:\n",
    "$$\n",
    "P(c = 1 | \\mathbf{X}, \\mathbf{c}, \\mathbf{x}_{new}) = \\frac{P(\\mathbf{x}_{new} | c = 1, \\mathbf{X}, \\mathbf{c}) P(c = 1)}{p(\\mathbf{x}_{new} | c = 1, \\mathbf{X}, \\mathbf{c}) P(c = 1) + p(\\mathbf{x}_{new} | c = 0, \\mathbf{X}, \\mathbf{c}) P(c = 0)} \\\\\n",
    "= \\frac{P(\\text{Not rich} | c = 1) P(\\text{Married} | c = 1) P(\\text{Healthy} | c = 1) P(c = 1)}{P(\\text{Not rich} | c = 1) P(\\text{Married} | c = 1) P(\\text{Healthy} | c = 1) P(c = 1) + P(\\text{Not rich} | c = 0) P(\\text{Married} | c = 0) P(\\text{Healthy} | c = 0) P(c = 0)} \\\\\n",
    "= \\frac{\\frac{1}{4} \\frac{1}{2} \\frac{1}{2} \\frac{1}{2}}{\\frac{1}{4} \\frac{1}{2} \\frac{1}{2} \\frac{1}{2} + \\frac{3}{4} \\frac{1}{4} \\frac{1}{2} \\frac{1}{2}} = \\frac{\\frac{1}{32}}{\\frac{1}{32} + \\frac{3}{64}} = \\frac{\\frac{2}{64}}{\\frac{2}{64} + \\frac{3}{64}} = \\frac{\\frac{2}{64}}{\\frac{5}{64}} = \\frac{2 \\cdot 64}{5 \\cdot 64} = \\frac{2}{5}.\n",
    "$$\n",
    "\n",
    "So the answer is $\\frac{2}{5}$.\n",
    "\n",
    "\n",
    "2.\n",
    "Here we are given $\\mathbf{x}_{new} = (0,1,\\sim)$, where $\\sim$ is a varible that is not defined (could be either 0 or 1). We can calculate this by using the probability when this varible is undefined:\n",
    "\n",
    "$$\n",
    "P(c = 1 | \\mathbf{X}, \\mathbf{c}, \\mathbf{x}_{new}) = \\frac{P(\\mathbf{x}_{new} | c = 1, \\mathbf{X}, \\mathbf{c}) P(c = 1)}{p(\\mathbf{x}_{new} | c = 1, \\mathbf{X}, \\mathbf{c}) P(c = 1) + p(\\mathbf{x}_{new} | c = 0, \\mathbf{X}, \\mathbf{c}) P(c = 0)} \\\\\n",
    "= \\frac{P(\\text{Not rich} | c = 1) P(\\text{Married} | c = 1) P(c = 1)}{P(\\text{Not rich} | c = 1) P(\\text{Married} | c = 1) P(c = 1) + P(\\text{Not rich} | c = 0) P(\\text{Married} | c = 0) P(c = 0)} \\\\\n",
    "= \\frac{\\frac{1}{4} \\frac{1}{2} \\frac{1}{2}}{\\frac{1}{4} \\frac{1}{2} \\frac{1}{2} + \\frac{3}{4} \\frac{1}{4} \\frac{1}{2}} = \\frac{\\frac{1}{16}}{\\frac{1}{16} + \\frac{3}{32}} = \\frac{\\frac{2}{32}}{\\frac{2}{32} + \\frac{3}{32}} = \\frac{\\frac{2}{32}}{\\frac{5}{32}} = \\frac{2 \\cdot 32}{5 \\cdot 32} = \\frac{2}{5}.\n",
    "$$\n",
    "\n",
    "So the answer here is also $\\frac{2}{5}$, this is resonable since the varible ''Healthy'' is in $c=0$ and $c=1$ equal amount of times and should not affect the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FkqFQkN1LWx"
   },
   "source": [
    "### Exercise  2.2 (3 pts)\n",
    "Naive Bayes refers to the classifier which we now describe. We consider here **binary** classification problem with **real valued data** i.e. $x \\in \\mathbb{R}^2$.\n",
    "#### 1. (1 pt)\n",
    "Assume that the class conditional density is **spherical** Gaussian, that is, the likelihood of the training(and testing) data $X, y$ given class $i$ can be expressed as: \n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new}, X, y) = P(x_{new} | \\tag{1}\n",
    "\\mu_{i}, \\Sigma_{i})\n",
    "$$\n",
    "\n",
    "Assume both classes have equal prior $p(y= \\pm 1) = 0.5$. Write the expression for the **naive Bayes** classifier, that is, derive\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) \\\\ \\tag{2}\n",
    "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
    "$$\n",
    "\n",
    "***Hint***: Derive the expressions of MLE for parameters in terms of training-data. Then express eq.1 in terms of those estimates using Bayes rule. \n",
    "\n",
    "#### 2. (2 pts)\n",
    "Derive the MLE expression for parameters when the covariance matrix is not diagonal, i.e, Covariance matrix has 4 unknown scalars. This is done to alleviate \"naive\" assumption, since now feature components are no longer independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nyg8jlaj1Nb8"
   },
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3iVQypENwJQ"
   },
   "source": [
    "## 3. [SVM, 5 points]\n",
    "\n",
    "### Excercise 3.1 (2 pts)\n",
    "\n",
    "Consider a (hard margin) SVM with the following training points from\n",
    "two classes:\n",
    "\\begin{eqnarray}\n",
    "+1: &(2,2), (4,4), (4,0) \\nonumber \\\\\n",
    "-1: &(0,0), (2,0), (0,2) \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "1. Plot these six training points, and construct, by inspection, the\n",
    "weight vector for the optimal hyperplane. **(1 pt)**\n",
    "\n",
    "2. In your solution, specify the hyperplane in terms of w and b such that $w_1 x_1 + w_2 x_2 + b =0$. Calculate the margin, i.e. $2\\gamma$, where $\\gamma$ is the\n",
    "distance from the hyperplane to its closest data point. (Hint: It may be useful to recall that the distance of a point $(a_1,a_2)$ from the line $w_1x_1 + w_2x_2 + b = 0$ is $|w_1a_1 + w_2a_2 + b|/\\sqrt{w_1^2 + w_2^2}$.) **(1 pt)**\n",
    "\n",
    "### Excercise 3.2 (3 pts)\n",
    "\n",
    "Consider the same problem from above.\n",
    "\n",
    "1. Write the primal formulation of the SVM **for this specific example** i.e. you have to specialise the general formulation for the set of inputs given. **(1 pt)**\n",
    "\n",
    "2. Write the dual formulation **for this specific**. Give the optimal dual solution, comment on support vectors. **(2 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data1 = np.array([[2,4,4],[2,4,0]])\n",
    "data2 = np.array([[0,2,0],[0,0,2]])\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.scatter(data1[0,:], data1[1,:], color = 'red', marker = 's', label = '+1')\n",
    "ax.scatter(data2[0,:], data2[1,:], color = 'blue', marker = '^', label = '-1')\n",
    "\n",
    "x = np.linspace(-1,5)\n",
    "y = 3-x\n",
    "ax.plot(x,y, color = 'green', label = 'Hyperplane: $0=-x_1-x_2+3$')\n",
    "ax.set(xlabel = '$x_1$', ylabel = '$x_2$')\n",
    "ax.set_title('Data set and hyperplane')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZWt7B2y1Qqx"
   },
   "source": [
    "### Your answer here:\n",
    "\n",
    "3.1.1.\n",
    "Data set can be seen in figure above, the figure also shows the hyperplane wich is $0=-x_1-x_2+3$, from equation $0=w_1x_1+w_2x_2+b$ where $w_1=w_2=-1$ and $b=3$.\n",
    "\n",
    "3.1.2.\n",
    "We have 4 points of interest to apply to the equation:\n",
    "\n",
    "$$\n",
    "\\gamma = \\frac{|w_1a_1+w_2a_2+b|}{\\sqrt{w_1^2+w_2^2}},\n",
    "$$\n",
    "we have $w_1$, $w_2$ and $b$ from 1. and the points we will look at is: $(2,2), (4,0), (2,0), (0,2)$ since they are clearly closest to the hyperplane:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "2\\gamma_1 &= \\frac{|-1 \\cdot 2 + (-1) \\cdot 2 + 3|}{\\sqrt{(-1)^2+(-1)^2}} = \\frac{|- 2 - 2 + 3|}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}} \\\\\n",
    "2\\gamma_2 &= \\frac{|-1 \\cdot 4 + (-1) \\cdot 0 + 3|}{\\sqrt{(-1)^2+(-1)^2}} = \\frac{|- 4 + 0 + 3|}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}} \\\\\n",
    "2\\gamma_3 &= \\frac{|-1 \\cdot 2 + (-1) \\cdot 0 + 3|}{\\sqrt{(-1)^2+(-1)^2}} = \\frac{|- 2 - 0 + 3|}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}} \\\\\n",
    "2\\gamma_4 &= \\frac{|-1 \\cdot 0 + (-1) \\cdot 2 + 3|}{\\sqrt{(-1)^2+(-1)^2}} = \\frac{|0 - 2 + 3|}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And they are all at the same distance to the hyperplane.\n",
    "\n",
    "3.2.1.\n",
    "The primal formulation for linear SVM is given by the maximisation of the margin $\\gamma = \\frac{1}{|\\mathbf{w}|}$:\n",
    "\n",
    "$$\n",
    "\\max_{\\mathbf{w}} 1/|\\mathbf{w}| = \\min_{\\mathbf{w}} |\\mathbf{w}| \\Rightarrow \\min_{\\mathbf{w}} \\frac{1}{2}\\mathbf{w}^\\top \\mathbf{w} = \\min_{\\mathbf{w}} \\mathbf{w}^\\top \\mathbf{w},\n",
    "$$\n",
    "\n",
    "with the constraints:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "t_n = 1: \\quad &\\mathbf{w}^\\top \\mathbf{x}_n + b \\geq 1 \\\\\n",
    "t_n = -1: \\quad &\\mathbf{w}^\\top \\mathbf{x}_n + b \\leq -1.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For our specific case we have for $t_n=1$:\n",
    "\n",
    "$$\n",
    "[w_1 w_2 ]\n",
    "\\begin{bmatrix}\n",
    "2\\\\ \n",
    "2\n",
    "\\end{bmatrix}\n",
    "+ b \\geq 1 \\\\\n",
    "[w_1 w_2 ]\n",
    "\\begin{bmatrix}\n",
    "4\\\\ \n",
    "4\n",
    "\\end{bmatrix}\n",
    "+ b \\geq 1 \\\\\n",
    "[w_1 w_2 ]\n",
    "\\begin{bmatrix}\n",
    "4\\\\ \n",
    "0\n",
    "\\end{bmatrix}\n",
    "+ b \\geq 1\n",
    "$$\n",
    "\n",
    "and for $t_n = -1$:\n",
    "\n",
    "$$\n",
    "[w_1 w_2 ]\n",
    "\\begin{bmatrix}\n",
    "0\\\\ \n",
    "0\n",
    "\\end{bmatrix}\n",
    "+ b \\leq -1 \\\\\n",
    "[w_1 w_2 ]\n",
    "\\begin{bmatrix}\n",
    "2\\\\ \n",
    "0\n",
    "\\end{bmatrix}\n",
    "+ b \\leq -1 \\\\\n",
    "[w_1 w_2 ]\n",
    "\\begin{bmatrix}\n",
    "0\\\\ \n",
    "2\n",
    "\\end{bmatrix}\n",
    "+ b \\leq -1.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2.\n",
    "\n",
    "The dual formulation of the above primal optimisation problem is given by\n",
    "\n",
    "$$\n",
    "\\max_{\\mathbf{\\alpha}} \\sum_{n=1}^6 \\alpha_n - \\frac{1}{2} \\sum_{n=1}^6 \\sum_{n=1}^6 \\alpha_n \\alpha_m t_n t_m \\mathbf{x}_n^\\top \\mathbf{x}_m\n",
    "$$\n",
    "$$\n",
    "\\textrm{s.t. } \\sum_{n=1}^6 \\alpha_n t_n = 0\n",
    "$$\n",
    "$$\n",
    "\\alpha_n \\geq 0, \\; n = 1, \\dots, 6\n",
    "$$\n",
    "\n",
    "for $\\mathbf{x}_1 = (2,2)$, $\\mathbf{x}_2 = (4,4)$, $\\mathbf{x}_3 = (4,0)$, $\\mathbf{x}_4 = (0,0)$, $\\mathbf{x}_5 = (2,0)$, $\\mathbf{x}_6 = (0,2)$\n",
    "\n",
    "and $t_1 = t_2 = t_3 = +1 $ while $t_4 = t_5 = t_6 = -1$.\n",
    "\n",
    "$\\alpha_n, \\, n=1, \\dots 6$ are the lagrangian dual variables corresponding to the 6 constraints in the primal problem. They express the rate of change in the margin from changing the relaxed constraints. The support vectors are the previously mentioned 4 points of interest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhHF2_-R00si"
   },
   "source": [
    "# Practical Question\n",
    "## 4. Logistic Regression (5 pts)\n",
    "### Customer churn with Logistic Regression\n",
    "A telecommunications company is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is leaving. Imagine that you are an analyst at this company and you have to find out who is leaving and why.\n",
    "\n",
    "### About the dataset\n",
    "\n",
    "We will use a telecommunications dataset for predicting customer churn. This is a historical customer dataset where each row represents one customer. The data is relatively easy to understand, and you may uncover insights you can use immediately. Typically it is less expensive to keep customers than acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company.\n",
    "This data set provides information to help you predict what behavior will help you to retain customers. You can analyse all relevant customer data and develop focused customer retention programs.\n",
    "The dataset includes information about:\n",
    "*   Customers who left within the last month – the column is called Churn.\n",
    "*   Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies.\n",
    "*   Customer account information – how long they had been a customer, contract, payment method, paperless billing, monthly charges, and total charges.\n",
    "*   Demographic info about customers – gender, age range, and if they have partners and dependents.\n",
    "We will help you load and visualise the dataset as well as the preprocessing, you need to build up your logistic regression model step by step and do the prediction.\n",
    "*   **Remember, you are not allowed to use sklearn in modelling and predicting, you have to fill your code in the skeleton.** \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QR5cEzzzOVba"
   },
   "outputs": [],
   "source": [
    "## Load the dataset and read it\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "import urllib\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve('https://raw.githubusercontent.com/BruceZHANG6/Imagesforuse/master/ChurnData.csv', 'ChurnData.csv')\n",
    "except urllib.error.HTTPError as ex:\n",
    "    print('Problem:', ex)\n",
    "    \n",
    "churn_df = pd.read_csv(\"ChurnData.csv\")\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2gDgP27WNFV"
   },
   "outputs": [],
   "source": [
    "## Data pre-processing and selection\n",
    "## Train/Test dataset split\n",
    "churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\n",
    "churn_df['churn'] = churn_df['churn'].astype('int')\n",
    "churn_df.head()\n",
    "\n",
    "X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\n",
    "y = np.asarray(churn_df['churn'])\n",
    "y = np.reshape(y, (np.asarray(churn_df['churn']).shape[0], 1))\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "X_train=X_train.T\n",
    "X_test=X_test.T\n",
    "y_train=y_train.T\n",
    "y_test=y_test.T\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMzOgQsCoJSN"
   },
   "source": [
    "**Hints**:\n",
    "- You compute $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ .\n",
    "- You compute activation $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$.\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$.\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$\n",
    "\n",
    "- You write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate.\n",
    "- In prediction, you calculate $\\hat{Y} = A = \\sigma(w^T X + b)$.\n",
    "- You may use np.exp, np.log(), np.dot(), etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCZ4BQ-uW6hg"
   },
   "outputs": [],
   "source": [
    "## Modeling and predicting\n",
    "\n",
    "# GRADED FUNCTION: sigmoid\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Return: s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = #Your code here\n",
    "    \n",
    "    return s\n",
    "  \n",
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized to 0\n",
    "    \"\"\"    \n",
    "    w = #Your code here\n",
    "    b = #Your code here\n",
    "   \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "  \n",
    "# GRADED FUNCTION: grad_cost\n",
    "def grad_cost(w, b, X, Y):\n",
    "  \"\"\"\n",
    "    Arguments:\n",
    "    X -- data of size (number of features, number of examples)\n",
    "    Y -- true \"label\" vector\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "  \"\"\" \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A = #Your code here                                   \n",
    "    cost = #Your code here                         \n",
    "    dw = #Your code here\n",
    "    db = #Your code here\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    " # GRADED FUNCTION: optimize\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = grad_cost(w,b,X,Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w = #Your code here\n",
    "        b = #Your code here\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)     \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}   \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "  \n",
    "# GRADED FUNCTION: predict\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    A = #Your code here\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        if A[0,i]<=0.5:\n",
    "            Y_prediction[0,i]=0;\n",
    "        if A[0,i]>0.5:\n",
    "            Y_prediction[0,i]=1\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction\n",
    "  \n",
    "# GRADED FUNCTION: model\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    Returns: d -- dictionary containing information about the model.\n",
    "    \"\"\"    \n",
    "    # initialize parameters with zeros\n",
    "    w, b = #Your code here\n",
    "\n",
    "    # Gradient descent\n",
    "    parameters, grads, costs = #Your code here\n",
    "    \n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "   \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wl7F4KSwZo9T"
   },
   "outputs": [],
   "source": [
    "## The train accuracy and test accuracy\n",
    "## Feel free to change the hyperparameters\n",
    "d = model(X_train, y_train, X_test, y_test, num_iterations = 20000, learning_rate = 0.005, print_cost = True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}